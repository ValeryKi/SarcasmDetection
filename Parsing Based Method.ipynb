{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from textblob.en.parsers import PatternParser\n",
    "import re\n",
    "from textblob.taggers import PatternTagger\n",
    "import ahocorasick\n",
    "import esmre\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import esm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "stdin, stdout = sys.stdin, sys.stdout\n",
    "reload(sys)\n",
    "sys.stdin, sys.stdout = stdin, stdout\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarc = pd.read_csv(\"data/SarcasmAddData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_set(filename):\n",
    "    data = pd.read_csv(filename, encoding=\"utf-8\", engine='python')\n",
    "    data = data[[\"text\", \"sarc\"]]\n",
    "    data = data[data[\"text\"].notnull()]\n",
    "    data = data[data[\"sarc\"].notnull()]\n",
    "    data[\"sarc\"] = data[\"sarc\"].apply(lambda x: int(x))\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    sarc = data[data[\"sarc\"]==1]\n",
    "    not_sarc = data[data[\"sarc\"]==0][:len(data[data[\"sarc\"]==1])]\n",
    "    data_1_1 = pd.concat([sarc, not_sarc], ignore_index=True)\n",
    "    data_1_1 = shuffle(data_1_1)\n",
    "    return data_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_strength_dict(f, val):\n",
    "    s_d = {}\n",
    "    s_file = open(f, \"r\")\n",
    "    for line in s_file:\n",
    "        l = line.split()\n",
    "        if val:\n",
    "            s_d[re.sub(r\"\\*\",r\"\",l[0])] = int(l[1])\n",
    "        else:\n",
    "            s_d[re.sub(r\"\\*\",r\"\",l[0])] = 0\n",
    "    return s_d\n",
    "\n",
    "sentiments_dict = get_strength_dict(\"sp_files/SentimentLookupTable.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def senti_score(x):\n",
    "    wl = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    if x in sentiments_dict:\n",
    "        return sentiments_dict[x]\n",
    "    lemma  = wl.lemmatize(x)\n",
    "    if lemma in sentiments_dict:\n",
    "        return sentiments_dict[lemma]\n",
    "    stem = ps.stem(x)\n",
    "    if stem in sentiments_dict:\n",
    "        return sentiments_dict[stem]\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_data_set(\"sarcasm_set_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ParserSarcasmDetector:\n",
    "        \n",
    "    def __init__(self,d):\n",
    "        self.d = d\n",
    "        self.positive_sentiments = esm.Index()\n",
    "        self.negative_sentiments = esm.Index()\n",
    "        self.positive_situations = esm.Index()\n",
    "        self.negative_situations = esm.Index()\n",
    "        self.s_positive_sentiments = set()\n",
    "        self.s_negative_sentiments = set()\n",
    "        self.s_positive_situations = set()\n",
    "        self.s_negative_situations = set()\n",
    "         \n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.pblga(data)\n",
    "        \n",
    "    def predict(self, data, d):\n",
    "        self.d = d\n",
    "        predictions = []\n",
    "        for t in data:\n",
    "            p = self.iws(t)\n",
    "            if p is not None:\n",
    "                predictions.append(p)\n",
    "            else:\n",
    "                ps = self.positive_sentiments.query(t.lower())\n",
    "                ns = self.negative_situations.query(t.lower())\n",
    "                if self.check_minimal_distance(ps, ns, t):\n",
    "                    predictions.append(1)\n",
    "                    continue\n",
    "                ps = self.positive_situations.query(t.lower())\n",
    "                ns = self.negative_sentiments.query(t.lower())\n",
    "                if self.check_minimal_distance(ps, ns, t):\n",
    "                    predictions.append(1)\n",
    "                    continue\n",
    "                predictions.append(0)\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def predict_simple(self, data):\n",
    "        predictions = []\n",
    "        for t in data:\n",
    "            p = self.iws(t)\n",
    "            if p is not None:\n",
    "                predictions.append(p)\n",
    "            else:\n",
    "                ps = self.positive_sentiments.query(t.lower())\n",
    "                ns = self.negative_situations.query(t.lower())\n",
    "                if (len(ps) > 0) and (len(ns) > 0):\n",
    "                    predictions.append(1)\n",
    "                    continue\n",
    "                ps = self.positive_situations.query(t.lower())\n",
    "                ns = self.negative_sentiments.query(t.lower())\n",
    "                if (len(ps) > 0) and (len(ns) > 0):\n",
    "                    predictions.append(1)\n",
    "                    continue\n",
    "                predictions.append(0)\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def check_minimal_distance(self, ps, ns, st):\n",
    "        for w1 in ps:\n",
    "            for w2 in ns:\n",
    "                m_w1 = re.search(r'\\b'+re.sub(r\"\\[|\\]\", r\"\",w1[1])+r'\\b', st.lower())\n",
    "                m_w2 = re.search(r'\\b'+re.sub(r\"\\[|\\]\", r\"\",w2[1])+r'\\b', st.lower())\n",
    "                if (m_w1 is not None) and (m_w2 is not None):\n",
    "                    w = w1[1].count(\" \") + w2[1].count(\" \")+w1[1].count(\"\\n\") + w2[1].count(\"\\n\")\n",
    "                    s1 = m_w1.start()\n",
    "                    e1 = m_w1.end()-1\n",
    "                    s2 = m_w2.start()\n",
    "                    e2 = m_w2.end()-1 \n",
    "                    if s1 < s2:\n",
    "                        s = s1\n",
    "                    else:\n",
    "                        s = s2\n",
    "                    if e1 > e2:\n",
    "                        e = e1\n",
    "                    else:\n",
    "                        e = e2\n",
    "                    sl = st[s:e+1]\n",
    "                    w = sl.count(\" \") + sl.count(\"\\n\") - w\n",
    "                    if w > 0 and w < self.d:\n",
    "                        return True\n",
    "        return False\n",
    "                    \n",
    "                \n",
    "\n",
    "    def pblga(self, data):\n",
    "        pf = set()\n",
    "        sentiments = set()\n",
    "        situations = set()\n",
    "        for t in data:\n",
    "            pf.add(TextBlob(t.lower()).parse())\n",
    "        for tw in pf:\n",
    "            t = tw.split()\n",
    "            t = [item for sublist in t for item in sublist]\n",
    "            phrases = self.get_phrase(t)\n",
    "            for ind, t in enumerate(phrases):\n",
    "                if (t[1] == \"NP\") or (t[1] == \"ADJP\"):\n",
    "                    sentiments.add(t[0])\n",
    "                if (t[1] == \"NP\") and  (ind < len(phrases) -1) and (phrases[ind+1][1] == \"VP\"):\n",
    "                    sentiments.add(t[0] + \" \" + phrases[ind+1][0])\n",
    "                if t[1] == \"VP\":\n",
    "                    situations.add(t[0])\n",
    "                if (ind < len(phrases) - 1) and (((t[1] == \"ADVP\") and  (phrases[ind+1][1] == \"VP\"))\n",
    "                    or ((t[1] == \"ADJP\") and  (phrases[ind+1][1] == \"VP\")) or\n",
    "                    ((t[1] == \"VP\") and  (phrases[ind+1][1] == \"NP\"))):\n",
    "                    situations.add(t[0] + \" \" + phrases[ind+1][0])\n",
    "                if (ind < len(phrases) - 2) and (((t[1] == \"VP\") and (phrases[ind+1][1] == \"ADVP\")\n",
    "                    and (phrases[ind+2][1] == \"ADJP\")) or\n",
    "                    ((t[1] == \"VP\") and (phrases[ind+1][1] == \"ADJP\")\n",
    "                    and (phrases[ind+2][1] == \"NP\")) or\n",
    "                    ((t[1] == \"ADVP\") and (phrases[ind+1][1] == \"ADJP\")\n",
    "                    and (phrases[ind+2][1] == \"NP\"))):\n",
    "                    situations.add(t[0] + \" \" + phrases[ind+1][0] + \" \" + phrases[ind+2][0])\n",
    "        for p in sentiments:\n",
    "            ss = self.get_sentiment_score(p)\n",
    "            if ss > 0:\n",
    "                if p not in self.s_positive_sentiments:\n",
    "                    self.s_positive_sentiments.add(p)\n",
    "                    self.positive_sentiments.enter(p)\n",
    "            elif ss < 0:\n",
    "                if p not in self.s_negative_sentiments:\n",
    "                    self.s_negative_sentiments.add(p)\n",
    "                    self.negative_sentiments.enter(p)\n",
    "        for p in situations:\n",
    "            ss = self.get_sentiment_score(p)\n",
    "            if ss > 0:\n",
    "                if p not in self.s_positive_situations:\n",
    "                    self.s_positive_situations.add(p)\n",
    "                    self.positive_situations.enter(p)\n",
    "            elif ss < 0:\n",
    "                if p not in self.s_negative_situations:\n",
    "                    self.s_negative_situations.add(p)\n",
    "                    self.negative_situations.enter(p)\n",
    "        \n",
    "        \n",
    "    def fix(self):\n",
    "        self.positive_sentiments.fix()\n",
    "        self.positive_situations.fix()\n",
    "        self.negative_sentiments.fix()\n",
    "        self.negative_situations.fix()\n",
    "    \n",
    "    \n",
    "    def get_phrase(self, text):\n",
    "        np = re.compile('.*NP')\n",
    "        vp = re.compile('.*VP')\n",
    "        advp = re.compile('.*ADVP')\n",
    "        adjp = re.compile('.*ADJP')\n",
    "        pred = None\n",
    "        cur = None\n",
    "        phrases = []\n",
    "        phrase = \"\"\n",
    "        for t in text:\n",
    "            if np.match(t[2]):\n",
    "                cur = \"NP\"\n",
    "            elif vp.match(t[2]):\n",
    "                cur = \"VP\"\n",
    "            elif advp.match(t[2]):\n",
    "                cur = \"ADVP\"\n",
    "            elif adjp.match(t[2]):\n",
    "                cur = \"ADJP\"\n",
    "            else:\n",
    "                cur = None\n",
    "            if pred != None:\n",
    "                phrases.append((phrase, pred))\n",
    "                phrase = \"\"\n",
    "                pred = None\n",
    "            if cur == pred:\n",
    "                phrase = phrase + \" \" + t[0]\n",
    "            elif pred == None:\n",
    "                pred = cur\n",
    "                phrase = t[0]\n",
    "            else:\n",
    "                phrases.append((phrase, pred))\n",
    "                pred = cur\n",
    "                phrase = t[0]\n",
    "        if pred != None:\n",
    "            phrases.append((phrase, pred))\n",
    "        return phrases\n",
    "    \n",
    "    def get_sentiment_score(self, text):\n",
    "        text = text.split()\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        for word in text:\n",
    "            s = []\n",
    "            sn = TextBlob(word.lower()+\" \").polarity\n",
    "            if sn != 0:\n",
    "                s.append(sn * 5)\n",
    "            ss = senti_score(word.lower())\n",
    "            if ss != 0:\n",
    "                s.append(sn)\n",
    "            polarity = np.mean(s or [0])\n",
    "            if polarity > 0:\n",
    "                positive += 1\n",
    "            elif polarity < 0:\n",
    "                negative += 1\n",
    "        pr = positive * 1.0 / len(text)\n",
    "        nr = negative * 1.0 / len(text)\n",
    "        return pr - nr\n",
    "    \n",
    "    def iws(self, text):\n",
    "        verbs = re.compile('VB*')\n",
    "        nouns = re.compile('NN*')\n",
    "        adjectives = re.compile('JJ*')\n",
    "        adverbs = re.compile('RB*')\n",
    "        blob = TextBlob(text, pos_tagger=PatternTagger())\n",
    "        tags = blob.pos_tags\n",
    "        for index, t in enumerate(tags):\n",
    "            if t[1]=='UH':\n",
    "                if (index < len(tags)-1) and (adjectives.match(tags[index+1][1]) or\n",
    "                                              adverbs.match(tags[index+1][1])):\n",
    "                    return 1\n",
    "                for i, next_tag in enumerate(tags[index+1:]):\n",
    "                    if (i < len(tags[index+1:]) - 1) and ((adverbs.match(next_tag[1]) \n",
    "                                                       and adjectives.match(tags[index+1+i+1][1]))or\n",
    "                     (adjectives.match(next_tag[1]) and nouns.match(tags[index+1+i+1][1])) or\n",
    "                     (adverbs.match(next_tag[1]) and verbs.match(tags[index+1+i+1][1])) ):\n",
    "                        return 1\n",
    "                return 0\n",
    "        return None  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trains_tests(data_f):\n",
    "    skf = KFold(len(data_f), n_folds=10, shuffle=False)\n",
    "    trains = []\n",
    "    tests = []\n",
    "    y_trains = []\n",
    "    y_tests = []\n",
    "    data_1_1 = shuffle(data_f)\n",
    "    for train, test in skf:\n",
    "        trains.append(data_f.iloc[train]) \n",
    "        tests.append(data_f.iloc[test])\n",
    "        y_trains.append(data_f.sarc.iloc[train])\n",
    "        y_tests.append(data_f.sarc.iloc[test])\n",
    "    return trains, tests, y_trains, y_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation_predict(d, trains, tests, ans_trains, ans_tests):\n",
    "    f_scores = []\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    accuracy_scores = []\n",
    "    predictions = []\n",
    "    for train, test, y_train, y_test in tqdm(zip(trains, tests, y_trains, y_tests)):\n",
    "        clf = ParserSarcasmDetector(7)\n",
    "        clf.fit(train[\"text\"][train[\"sarc\"]==1])\n",
    "        clf.fit(sarc[\"body\"])\n",
    "        clf.fix()\n",
    "        y_pred = clf.predict(test[\"text\"], d)\n",
    "        predictions.append(y_pred)\n",
    "        f_scores.append(f1_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred))\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "    return f_scores, recall_scores, precision_scores, accuracy_scores, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation_predict_simple(trains, tests, ans_trains, ans_tests):\n",
    "    f_scores = []\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    accuracy_scores = []\n",
    "    predictions = []\n",
    "    for train, test, y_train, y_test in tqdm(zip(trains, tests, y_trains, y_tests)):\n",
    "        clf = ParserSarcasmDetector(7)\n",
    "        clf.fit(train[\"text\"][train[\"sarc\"]==1])\n",
    "        clf.fit(sarc[\"body\"])\n",
    "        clf.fix()\n",
    "        y_pred = clf.predict_simple(test[\"text\"])\n",
    "        predictions.append(y_pred)\n",
    "        f_scores.append(f1_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred))\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "    return f_scores, recall_scores, precision_scores, accuracy_scores, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trains, tests, y_trains, y_tests = get_trains_tests(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1, recall, pr, acc, predict = evaluation_predict(16, trains, tests, y_trains, y_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.197028429731\n",
      "recall: 0.114998397273\n",
      "precision: 0.693111273482\n",
      "accuracy: 0.531983960449\n"
     ]
    }
   ],
   "source": [
    "print \"f1:\", np.mean(f1)\n",
    "print \"recall:\", np.mean(recall)\n",
    "print \"precision:\", np.mean(pr)\n",
    "print \"accuracy:\", np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1, recall, pr, acc, predict = evaluation_predict_simple(trains, tests, y_trains, y_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.306460189159\n",
      "recall: 0.202760035066\n",
      "precision: 0.629484430976\n",
      "accuracy: 0.541859884178\n"
     ]
    }
   ],
   "source": [
    "print \"f1:\", np.mean(f1)\n",
    "print \"recall:\", np.mean(recall)\n",
    "print \"precision:\", np.mean(pr)\n",
    "print \"accuracy:\", np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
