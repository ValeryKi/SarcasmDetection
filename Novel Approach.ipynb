{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "from sklearn.utils import shuffle\n",
    "from senticnet.senticnet import Senticnet\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from textblob import TextBlob\n",
    "from textblob.taggers import PatternTagger\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_set(filename):\n",
    "    data = pd.read_csv(filename, encoding=\"utf-8\", engine='python')\n",
    "    data = data[[\"text\", \"sarc\"]]\n",
    "    data = data[data[\"text\"].notnull()]\n",
    "    data = data[data[\"sarc\"].notnull()]\n",
    "    data[\"sarc\"] = data[\"sarc\"].apply(lambda x: int(x))\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    sarc = data[data[\"sarc\"]==1]\n",
    "    not_sarc = data[data[\"sarc\"]==0][:len(data[data[\"sarc\"]==1])]\n",
    "    data_1_1 = pd.concat([sarc, not_sarc], ignore_index=True)\n",
    "    data_1_1 = shuffle(data_1_1)\n",
    "    return data_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_data_set(\"sarcasm_set_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_anc_dict(f):\n",
    "    anc = defaultdict(dict)\n",
    "    anc_file = open(f, \"r\")\n",
    "    for line in anc_file:\n",
    "        l = line.split()\n",
    "        anc[l[0]][l[2]] = int(l[3]) \n",
    "    return anc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anc_all_data = \"sp_files/ANC-all-lemma.txt\"\n",
    "anc_spoken_data = \"sp_files/ANC-spoken-lemma.txt\"\n",
    "anc_written_data = \"sp_files/ANC-written-lemma.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anc_all = get_anc_dict(anc_all_data)\n",
    "anc_written = get_anc_dict(anc_written_data)\n",
    "anc_spoken = get_anc_dict(anc_spoken_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intens = pd.read_csv(\"sp_files/wn-asr-multilevel-assess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = list(intens[\"Word\"])\n",
    "values = list(intens[\"NormedScore\"])\n",
    "int_scores = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tagging(text):\n",
    "    text_and_tags = PatternTagger().tag(text.decode(\"utf-8\").lower())\n",
    "    return text_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mean_and_rarest_fr(tagged_text):\n",
    "    arr = [int(anc_all[i[0]][i[1]]) for i in tagged_text if (i[0] in anc_all) and \n",
    "                    (i[1] in anc_all[i[0]])]\n",
    "    if len(arr) == 0:\n",
    "        return (0, 0)\n",
    "    mean = np.mean(arr)\n",
    "    minimum = np.min(arr)\n",
    "    return (mean, minimum)\n",
    "\n",
    "def get_frequency_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"mean_and_r_fr\"] = df[\"tagged\"].apply(lambda x: get_mean_and_rarest_fr(x))\n",
    "    df[\"mean_fr\"] = df[\"mean_and_r_fr\"].apply(lambda x: x[0])\n",
    "    df[\"r_fr\"] = df[\"mean_and_r_fr\"].apply(lambda x: x[1])\n",
    "    df = df.drop(\"mean_and_r_fr\", axis=1)\n",
    "    df[\"fr_gap\"] = abs(df[\"mean_fr\"] - df[\"r_fr\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mean_written(tagged_text):\n",
    "    arr = [int(anc_written[i[0]][i[1]]) for i in tagged_text if (i[0] in anc_written) and \n",
    "                    (i[1] in anc_written[i[0]])]\n",
    "    if len(arr) == 0:\n",
    "        return 0\n",
    "    return np.mean(arr)\n",
    "\n",
    "def get_mean_spoken(tagged_text):\n",
    "    arr = [int(anc_spoken[i[0]][i[1]]) for i in tagged_text if (i[0] in anc_spoken) and \n",
    "                    (i[1] in anc_spoken[i[0]])]\n",
    "    if len(arr) == 0:\n",
    "        return 0\n",
    "    return np.mean(arr)\n",
    "\n",
    "def get_style_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"m_w\"] = df[\"tagged\"].apply(lambda x: get_mean_written(x))\n",
    "    df[\"m_s\"] = df[\"tagged\"].apply(lambda x: get_mean_spoken(x))\n",
    "    df[\"w_s_gap\"] = abs(df[\"m_w\"] - df[\"m_s\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words_number(text):\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')\n",
    "    filtered = [w[0] for w in text if nonPunct.match(w[0])]\n",
    "    return len(filtered)\n",
    "\n",
    "def get_words_length_mean(text):\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')\n",
    "    filtered = [w[0] for w in text if nonPunct.match(w[0])]\n",
    "    return np.mean([len(w) for w in filtered] or [0])\n",
    "\n",
    "def get_verbs_number(text):\n",
    "    verbs = re.compile('VB*')\n",
    "    return len([t[1] for t in text if verbs.match(t[1])])\n",
    "\n",
    "def get_nouns_number(text):\n",
    "    nouns = re.compile('NN*')\n",
    "    return len([t[1] for t in text if nouns.match(t[1])])\n",
    "\n",
    "def get_adjectives_number(text):\n",
    "    adjectives = re.compile('JJ*')\n",
    "    return len([t[1] for t in text if adjectives.match(t[1])])\n",
    "    \n",
    "def get_adverbs_number(text):\n",
    "    adverbs = re.compile('RB*')\n",
    "    return len([t[1] for t in text if adverbs.match(t[1])])\n",
    "\n",
    "def div(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a * 1.0 / b\n",
    "    \n",
    "def get_stracture_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"length\"] = df[\"text\"].apply(lambda x: len(x))\n",
    "    df[\"words_number\"] = df[\"tagged\"].apply(lambda x: get_words_number(x))\n",
    "    df[\"words_length_mean\"] = df[\"tagged\"].apply(lambda x: get_words_length_mean(x))\n",
    "    df[\"verbs_number\"] = df[\"tagged\"].apply(lambda x: get_verbs_number(x))\n",
    "    df[\"nouns_number\"] = df[\"tagged\"].apply(lambda x: get_nouns_number(x))\n",
    "    df[\"adverbs_number\"] = df[\"tagged\"].apply(lambda x: get_adverbs_number(x))\n",
    "    df[\"adjectives_number\"] = df[\"tagged\"].apply(lambda x: get_adjectives_number(x))\n",
    "    df[\"verbs_ratio\"] = df[[\"verbs_number\", \"words_number\"]].apply(lambda x: div(x[0], x[1]), axis=1) \n",
    "    df[\"nouns_ratio\"] = df[[\"nouns_number\", \"words_number\"]].apply(lambda x: div(x[0], x[1]), axis=1) \n",
    "    df[\"adverbs_ratio\"] = df[[\"adverbs_number\", \"words_number\"]].apply(lambda x: div(x[0], x[1]), axis=1) \n",
    "    df[\"adjectives_ratio\"] = df[[\"adjectives_number\", \"words_number\"]].apply(lambda x: div(x[0], x[1]), axis=1) \n",
    "    df[\"laughing_number\"] = df[\"text\"].apply(lambda x: x.lower().count(\"hahah\") + x.lower().count(\"lol\")\n",
    "                                             + x.lower().count(\"rofl\") + x.lower().count(\"lmao\"))\n",
    "    df[\"commas_number\"] = df[\"text\"].apply(lambda x: x.lower().count(','))\n",
    "    df[\"full_stops_number\"] = df[\"text\"].apply(lambda x: x.lower().count('.'))\n",
    "    df[\"ellipsis_number\"] = df[\"text\"].apply(lambda x: x.lower().count('...'))\n",
    "    df[\"exclamation_number\"] = df[\"text\"].apply(lambda x: x.lower().count('!'))\n",
    "    df[\"quatation_number\"] = df[\"text\"].apply(lambda x: x.lower().count('?'))\n",
    "    df[\"punctuation\"] = df[\"commas_number\"] + df[\"exclamation_number\"]\n",
    "    df[\"punctuation\"] = df[\"punctuation\"] + df[\"quatation_number\"] \n",
    "    df[\"punctuation\"] = df[\"punctuation\"] + df[\"ellipsis_number\"] \n",
    "    df[\"punctuation\"] = df[\"punctuation\"]+ df[\"full_stops_number\"]\n",
    "    df[\"emoticon\"] = df[\"text\"].apply(lambda x: x.lower().count(':)') + x.lower().count(':(') + x.lower().count(':D')\n",
    "                                      +x.lower().count(';)'))\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_adj_total_mean_max(text):\n",
    "    adjectives = re.compile('JJ*')\n",
    "    adjs = [t[0] for t in text if adjectives.match(t[1])]\n",
    "    scores = [int_scores[t+\"/a\"] for t in adjs if (t+\"/a\") in int_scores]\n",
    "    if len(scores) == 0:\n",
    "        return (0, 0, 0)\n",
    "    return (np.sum(scores), np.mean(scores), np.max(scores))\n",
    "\n",
    "def get_adv_total_mean_max(text):\n",
    "    adverbs = re.compile('RB*')\n",
    "    advbs = [t[0] for t in text if adverbs.match(t[1])]\n",
    "    scores = [int_scores[t+\"/r\"] for t in advbs if (t+\"/r\") in int_scores]\n",
    "    if len(scores) == 0:\n",
    "        return (0, 0, 0)\n",
    "    return (np.sum(scores), np.mean(scores), np.max(scores))\n",
    "\n",
    "def get_intensity_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"adj_t_mean_max\"] = df[\"tagged\"].apply(lambda x: get_adj_total_mean_max(x))\n",
    "    df[\"adj_total\"] = df[\"adj_t_mean_max\"].apply(lambda x: x[0])\n",
    "    df[\"adj_mean\"] = df[\"adj_t_mean_max\"].apply(lambda x: x[1])\n",
    "    df[\"adj_max\"] = df[\"adj_t_mean_max\"].apply(lambda x: x[2])\n",
    "    df[\"adj_gap\"] = abs(df[\"adj_max\"] - df[\"adj_mean\"])\n",
    "    df[\"adv_t_mean_max\"] = df[\"tagged\"].apply(lambda x: get_adv_total_mean_max(x))\n",
    "    df[\"adv_total\"] = df[\"adv_t_mean_max\"].apply(lambda x: x[0])\n",
    "    df[\"adv_mean\"] = df[\"adv_t_mean_max\"].apply(lambda x: x[1])\n",
    "    df[\"adv_max\"] = df[\"adv_t_mean_max\"].apply(lambda x: x[2])\n",
    "    df[\"adv_gap\"] = abs(df[\"adv_max\"] - df[\"adv_mean\"])\n",
    "    df = df.drop(\"adj_t_mean_max\", axis=1)\n",
    "    df = df.drop(\"adv_t_mean_max\", axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_synonyms(word, tag):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word, pos=tag):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    return list(set(synonyms))\n",
    "\n",
    "def get_text_syn_f(tagged_text):\n",
    "    words = {}\n",
    "    verbs = re.compile('VB*')\n",
    "    nouns = re.compile('NN*')\n",
    "    adjectives = re.compile('JJ*')\n",
    "    adverbs = re.compile('RB*')\n",
    "    for w in tagged_text:\n",
    "        if verbs.match(w[1]):\n",
    "            words[w[0]] = get_synonyms(w[0], wn.VERB)\n",
    "        if nouns.match(w[1]):\n",
    "            words[w[0]] = get_synonyms(w[0], wn.NOUN)\n",
    "        if adjectives.match(w[1]):\n",
    "            words[w[0]] = get_synonyms(w[0], wn.ADJ)\n",
    "        if adverbs.match(w[1]):\n",
    "            words[w[0]] = get_synonyms(w[0], wn.ADV)\n",
    "    sl_w = [len([s for s in words[w] if anc_all[s]< anc_all[w]]) for w in words.keys()]\n",
    "    s_mean = 0\n",
    "    if len(sl_w) != 0:\n",
    "        s_mean = np.mean(sl_w)\n",
    "    wls_t = max(sl_w or [0])\n",
    "    sg_w = [len([s for s in words[w] if anc_all[s] > anc_all[w]]) for w in words.keys()]\n",
    "    wgs_t = max(sg_w or [0])\n",
    "    s_l_gap = abs(wls_t - s_mean)\n",
    "    g_mean = 0\n",
    "    if len(sg_w) != 0:\n",
    "        g_mean = np.mean(sg_w)\n",
    "    s_g_gap =abs(wgs_t - g_mean)\n",
    "    return (s_mean, s_l_gap, g_mean, s_g_gap)\n",
    "    \n",
    "def get_synonyms_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"syn_smean_sgap_g_mean_g_gap\"] = df[\"tagged\"].apply(lambda x: get_text_syn_f(x))\n",
    "    df[\"s_mean\"] = df[\"syn_smean_sgap_g_mean_g_gap\"].apply(lambda x: x[0])\n",
    "    df[\"s_l_gap\"] = df[\"syn_smean_sgap_g_mean_g_gap\"].apply(lambda x: x[1])\n",
    "    df[\"g_mean\"] = df[\"syn_smean_sgap_g_mean_g_gap\"].apply(lambda x: x[2])\n",
    "    df[\"s_g_gap\"] = df[\"syn_smean_sgap_g_mean_g_gap\"].apply(lambda x: x[3])\n",
    "    df = df.drop(\"syn_smean_sgap_g_mean_g_gap\", axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ambiquity_text_f(tagged_text):\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')\n",
    "    words = [w[0] for w in tagged_text if nonPunct.match(w[0])]\n",
    "    synsets = [len(wn.synsets(w)) for w in words]\n",
    "    if len(synsets) == 0:\n",
    "        synset_mean = 0\n",
    "    else:\n",
    "        synset_mean = sum(synsets or [0]) * 1.0 / len(synsets)\n",
    "    synset_max = max(synsets or [0])\n",
    "    synset_gap = abs(synset_max - synset_mean)\n",
    "    return (synset_mean, synset_max, synset_gap)\n",
    "        \n",
    "def get_ambiguity_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"mmg\"] = df[\"tagged\"].apply(lambda x: get_ambiquity_text_f(x))\n",
    "    df[\"sysnset_mean\"] = df[\"mmg\"].apply(lambda x: x[0])\n",
    "    df[\"sysnset_max\"] = df[\"mmg\"].apply(lambda x: x[1])\n",
    "    df[\"sysnset_gap\"] = df[\"mmg\"].apply(lambda x: x[2])\n",
    "    df = df.drop(\"mmg\", axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_senti_synonyms(word, tag):\n",
    "    synonyms = []\n",
    "    for syn in swn.senti_synsets(word, pos=tag):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    return list(set(synonyms))\n",
    "\n",
    "def get_sentiment_f_from_text(tagged_text):\n",
    "    words = {}\n",
    "    verbs = re.compile('VB*')\n",
    "    nouns = re.compile('NN*')\n",
    "    adjectives = re.compile('JJ*')\n",
    "    adverbs = re.compile('RB*')\n",
    "    for w in tagged_text:\n",
    "        l = []\n",
    "        if verbs.match(w[1]):\n",
    "            l = swn.senti_synsets(w[0], wn.VERB)\n",
    "        if nouns.match(w[1]):\n",
    "            l = swn.senti_synsets(w[0], wn.NOUN)\n",
    "        if adjectives.match(w[1]):\n",
    "            l = swn.senti_synsets(w[0], wn.ADJ)\n",
    "        if adverbs.match(w[1]):\n",
    "            l = swn.senti_synsets(w[0], wn.ADV)\n",
    "        if len(l) > 0:\n",
    "            words[w[0]] = l[0]\n",
    "    pos_scores = [words[key].pos_score() for key in words.keys()]\n",
    "    neg_scores = [words[key].neg_score() for key in words.keys()]\n",
    "    pos_scores_sum = sum(pos_scores or [0])\n",
    "    neg_scores_sum = sum(neg_scores or [0])\n",
    "    avg_pos_scores = (pos_scores_sum + neg_scores_sum) * 1.0 / 2\n",
    "    pos_neg_gap = pos_scores_sum - neg_scores_sum\n",
    "    positive_single_gap = abs(pos_scores_sum - max(pos_scores or [0]))\n",
    "    negative_single_gap = abs(neg_scores_sum - max(neg_scores or [0]))\n",
    "    return (pos_scores_sum, neg_scores_sum, avg_pos_scores, pos_neg_gap, positive_single_gap, negative_single_gap)\n",
    "    \n",
    "    \n",
    "def get_sentiments_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"s\"] = df[\"tagged\"].apply(lambda x: get_sentiment_f_from_text(x))\n",
    "    df[\"pos_scores_sum\"] = df[\"s\"].apply(lambda x: x[0])\n",
    "    df[\"neg_scores_sum\"] = df[\"s\"].apply(lambda x: x[1])\n",
    "    df[\"avg_pos_scores\"] = df[\"s\"].apply(lambda x: x[2])\n",
    "    df[\"pos_neg_gap\"] = df[\"s\"].apply(lambda x: x[3])\n",
    "    df[\"positive_single_gap\"] = df[\"s\"].apply(lambda x: x[4])\n",
    "    df[\"negative_single_gap\"] = df[\"s\"].apply(lambda x: x[5])\n",
    "    df = df.drop(\"s\", axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_set_novel(df):\n",
    "    df = df.copy()\n",
    "    df[\"tagged\"] = df[\"text\"].apply(lambda x: get_tagging(x))\n",
    "    df = get_frequency_f(df)\n",
    "    df = get_style_f(df)\n",
    "    df = get_stracture_f(df)\n",
    "    df = get_intensity_f(df)\n",
    "    df = get_synonyms_f(df)\n",
    "    df = get_ambiguity_f(df)\n",
    "    df = get_sentiments_f(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_f = create_feature_set_novel(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_trains_tests(data_f):\n",
    "    skf = KFold(len(data_f), n_folds=10, shuffle=False, random_state=None)\n",
    "    trains = []\n",
    "    tests = []\n",
    "    y_trains = []\n",
    "    y_tests = []\n",
    "    data_1_1 = shuffle(data_f)\n",
    "    for train, test in skf:\n",
    "        trains.append(data_f.iloc[train]) \n",
    "        tests.append(data_f.iloc[test])\n",
    "        y_trains.append(data_f.sarc.iloc[train])\n",
    "        y_tests.append(data_f.sarc.iloc[test])\n",
    "    return trains, tests, y_trains, y_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tt_sets(trains, test, columns_f):\n",
    "    new_trains = []\n",
    "    new_tests = []\n",
    "    for train, test in tqdm(zip(trains, test)):\n",
    "        scaler = StandardScaler()\n",
    "        s_train = scaler.fit_transform(train[columns_f].astype(float))\n",
    "        s_test = scaler.transform(test[columns_f].astype(float))\n",
    "        new_trains.append(s_train)\n",
    "        new_tests.append(s_test)\n",
    "    return new_trains, new_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation(clf, trains, tests, ans_trains, ans_tests, proba):\n",
    "    f_scores = []\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    accuracy_scores = []\n",
    "    auc_scores = []\n",
    "    predictions = []\n",
    "    probas = []\n",
    "    for train, test, y_train, y_test in tqdm(zip(trains, tests, y_trains, y_tests)):\n",
    "        clf.fit(train, y_train)\n",
    "        y_pred = clf.predict(test)\n",
    "        if proba:\n",
    "            y_proba = clf.predict_proba(test)\n",
    "            probas.append(y_proba)\n",
    "            auc_scores.append(roc_auc_score(y_test, y_proba[:,1]))\n",
    "        predictions.append(y_pred)\n",
    "        f_scores.append(f1_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred))\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "    return f_scores, recall_scores, precision_scores, accuracy_scores, auc_scores, predictions, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trains, tests, y_trains, y_tests = get_trains_tests(data_f)\n",
    "clf = svm.SVC(C=2.1, probability=True)\n",
    "svm_features = data_f.columns.difference(set([\"text\", \"sarc\", \"tagged\"]))\n",
    "svm_trains, svm_tests = create_tt_sets(trains, tests, svm_features)\n",
    "svm_f1, svm_recall, svm_pr, svm_acc, svm_auc, svm_predict, svm_probas = evaluation(clf, svm_trains, svm_tests,\n",
    "                                                                       y_trains, y_tests, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm avg f1: 0.720279569172\n",
      "svm avg recall: 0.758680090855\n",
      "svm avg precision: 0.686152664105\n",
      "svm avg accuracy: 0.705633175631\n",
      "svm avg auc: 0.769866676989\n"
     ]
    }
   ],
   "source": [
    "print \"svm avg f1:\", np.mean(svm_f1)\n",
    "print \"svm avg recall:\", np.mean(svm_recall)\n",
    "print \"svm avg precision:\", np.mean(svm_pr)\n",
    "print \"svm avg accuracy:\", np.mean(svm_acc)\n",
    "print \"svm avg auc:\", np.mean(svm_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trains, tests, y_trains, y_tests = get_trains_tests(data_f)\n",
    "clf = RandomForestClassifier(n_estimators=250, n_jobs=-1)\n",
    "rf_features = data_f.columns.difference(set([\"text\", \"sarc\", \"tagged\"]))\n",
    "rf_trains, rf_tests = create_tt_sets(trains, tests, rf_features)\n",
    "rf_f1, rf_recall, rf_pr, rf_acc, rf_auc, rf_predict, rf_probas = evaluation(clf, rf_trains, rf_tests,\n",
    "                                                                       y_trains, y_tests, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf avg f1: 0.743496364085\n",
      "rf avg recall: 0.802771964579\n",
      "rf avg precision: 0.693070667372\n",
      "rf avg accuracy: 0.723319909115\n",
      "rf avg auc: 0.795802187818\n"
     ]
    }
   ],
   "source": [
    "print \"rf avg f1:\", np.mean(rf_f1)\n",
    "print \"rf avg recall:\", np.mean(rf_recall)\n",
    "print \"rf avg precision:\", np.mean(rf_pr)\n",
    "print \"rf avg accuracy:\", np.mean(rf_acc)\n",
    "print \"rf avg auc:\", np.mean(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_sets():\n",
    "    trains = []\n",
    "    tests = []\n",
    "    y_trains = []\n",
    "    y_tests = []\n",
    "    for i in tqdm(range(10)):\n",
    "        train = pd.read_csv(\"data_train_kf_\"+str(i)+\".csv\", encoding=\"utf-8\")\n",
    "        test = pd.read_csv(\"data_test_kf_\"+str(i)+\".csv\", encoding=\"utf-8\")\n",
    "        x_train = create_feature_set_novel(train)\n",
    "        trains.append(x_train)\n",
    "        y_train = x_train.sarc\n",
    "        y_trains.append(y_train)\n",
    "        x_test = create_feature_set_novel(test)\n",
    "        tests.append(x_test)\n",
    "        y_test = x_test.sarc\n",
    "        y_tests.append(y_test)\n",
    "    return trains, tests, y_trains, y_tests  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trains, tests, y_trains, y_tests = load_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=250, n_jobs=-1)\n",
    "rf_features = trains[0].columns.difference(set([\"text\", \"sarc\", \"tagged\"]))\n",
    "rf_trains, rf_tests = create_tt_sets(trains, tests, rf_features)\n",
    "rf_f1, rf_recall, rf_pr, rf_acc, rf_auc, rf_predict, rf_probas = evaluation(clf, rf_trains, rf_tests,\n",
    "                                                                       y_trains, y_tests, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_answers(alg_name, prd, probas):\n",
    "    i = 0\n",
    "    for y_pr, y_proba in zip(prd, probas):\n",
    "        pd.Series(list(y_pr)).to_csv(alg_name +\"_y_prd_kf_\"+str(i)+\".csv\")\n",
    "        pd.Series(list(y_proba[:,1])).to_csv(alg_name +\"_y_prb_kf_\"+str(i)+\".csv\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store_answers(\"nov\", rf_predict, rf_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf avg f1: 0.743029425788\n",
      "rf avg recall: 0.798624161494\n",
      "rf avg precision: 0.695059829862\n",
      "rf avg accuracy: 0.724203127104\n",
      "rf avg auc: 0.796637798851\n"
     ]
    }
   ],
   "source": [
    "print \"rf avg f1:\", np.mean(rf_f1)\n",
    "print \"rf avg recall:\", np.mean(rf_recall)\n",
    "print \"rf avg precision:\", np.mean(rf_pr)\n",
    "print \"rf avg accuracy:\", np.mean(rf_acc)\n",
    "print \"rf avg auc:\", np.mean(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
