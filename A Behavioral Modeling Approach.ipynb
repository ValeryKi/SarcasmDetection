{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.stats import mode\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "from numpy import std\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sps\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob.taggers import PatternTagger\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_dict = {\"CC\":1, \"CD\":2, \"DT\":3, \"EX\":4, \"FW\":5,\"IN\":6,\"JJ\":7,\n",
    "         \"JJR\":8, \"JJS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13,\n",
    "         \"NNPS\":14,\"PDT\":15, \"POS\":16, \"PRP\":17, \"PRP$\":18, \"RB\":19,\n",
    "         \"RBR\":20, \"RBS\":21,\"RP\":22,\"SYM\":23,\"TO\":24,\"UH\":25,\"VB\":26,\n",
    "         \"VBD\":27,\"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32,\n",
    "          \"WP\":33, \"WP$\":34, \"WRB\":35, 'NN|JJ':36}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_set(f):\n",
    "    s = set()\n",
    "    set_file = open(f, \"r\")\n",
    "    for line in set_file:\n",
    "        l = line.split()\n",
    "        s.add(l[0])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swear_words_set = get_set(\"sp_files/swear_words_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_set(filename):\n",
    "    data = pd.read_csv(filename, encoding=\"utf-8\", engine='python')\n",
    "    data = data[[\"text\", \"sarc\"]]\n",
    "    data = data[data[\"text\"].notnull()]\n",
    "    data = data[data[\"sarc\"].notnull()]\n",
    "    data[\"sarc\"] = data[\"sarc\"].apply(lambda x: int(x))\n",
    "    data = data.drop_duplicates()\n",
    "    sarc = data[data[\"sarc\"]==1]\n",
    "    not_sarc = data[data[\"sarc\"]==0][:len(data[data[\"sarc\"]==1])]\n",
    "    data_1_1 = pd.concat([sarc, not_sarc], ignore_index=True)\n",
    "    data_1_1 = shuffle(data_1_1)\n",
    "    return data_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_data_set(\"sarcasm_set.csv\")\n",
    "data_small = get_data_set(\"sarcasm_set_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affect = pd.read_csv(\"sp_files/Ratings_Warriner_et_al-2.csv\")\n",
    "keys = list(affect[\"Word\"])\n",
    "values = list(affect[\"V.Mean.Sum\"])\n",
    "affect_scores = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_anc_dict(f):\n",
    "    anc = defaultdict(dict)\n",
    "    anc_file = open(f, \"r\")\n",
    "    for line in anc_file:\n",
    "        l = line.split()\n",
    "        anc[l[0]][l[2]] = int(l[3]) \n",
    "    return anc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_strength_dict(f):\n",
    "    s_d = {}\n",
    "    s_file = open(f, \"r\")\n",
    "    for line in s_file:\n",
    "        l = line.split()\n",
    "        s_d[re.sub(r\"\\*\",r\"\",l[0])] = int(l[1]) \n",
    "    return s_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "booster_dict = get_strength_dict(\"sp_files/BoosterWordList.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments_dict = get_strength_dict(\"sp_files/SentimentLookupTable.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anc_all_data = \"sp_files/ANC-all-lemma.txt\"\n",
    "anc_all = get_anc_dict(anc_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tagging(text):\n",
    "    text_and_tags = PatternTagger().tag(text.decode(\"utf-8\").lower())\n",
    "    return text_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_distrbution(arr, n, k):\n",
    "    d = [0] * n\n",
    "    for i in arr: \n",
    "        d[int(round(i)) + k] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distribution_f(df, col):\n",
    "    df = df.copy()\n",
    "    df[col+\"_mean\"]= df[col].apply(lambda x: mean(x or [0]))\n",
    "    df[col+\"_median\"]= df[col].apply(lambda x: median(x or [0]))\n",
    "    df[col+\"_mode\"]= df[col].apply(lambda x: mode(x or [0],axis=None).mode[0])\n",
    "    df[col+\"_std\"]= df[col].apply(lambda x: std(x or [0]))\n",
    "    df[col+\"_max\"]= df[col].apply(lambda x: max(x or [0]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(x):\n",
    "    s = [senti_score(i[0]) for i in x]\n",
    "    return max(s or [0]) - min(s or [0])\n",
    "\n",
    "def get_affect_score(x):\n",
    "    s = [affect_scores[i[0]] for i in x if i[0] in affect_scores]\n",
    "    return max(s or [0]) - min(s or [0])\n",
    "\n",
    "def get_contrasting_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"del_sentiment\"] = df[\"tagged_words\"].apply(lambda x: get_sentiment(x))\n",
    "    df[\"del_affect\"] = df[\"tagged_words\"].apply(lambda x: get_affect_score(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_multi_f(df, col,n):\n",
    "    df = df.copy()\n",
    "    for i in range(n):\n",
    "        df[col + \"_\" + str(i)] = df[col].apply(lambda x: x[i])\n",
    "    df = df.drop(col, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_swear_words(t):\n",
    "    for w in t:\n",
    "        if w[0] in swear_words_set:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mood_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"sen_d\"] = df[\"tagged_words\"].apply(lambda x: [senti_score(i[0]) for i in x])\n",
    "    df[\"aff_d\"] = df[\"tagged_words\"].apply(lambda x: [affect_scores[i[0]] for i in x if i[0] in affect_scores])\n",
    "    df = get_distribution_f(df, \"sen_d\")\n",
    "    df = get_distribution_f(df, \"aff_d\")\n",
    "    df[\"d_s\"] = df[\"sen_d\"].apply(lambda x: get_distrbution([i for i in x], 11, 5))\n",
    "    df[\"d_a\"] = df[\"aff_d\"].apply(lambda x: get_distrbution(x, 9, -1))\n",
    "    df = get_multi_f(df, \"d_s\", 11)\n",
    "    df = get_multi_f(df, \"d_a\",9)\n",
    "    df[\"num_sen_words\"] = df[\"sen_d\"].apply(lambda x: len([i for i in x if i != 0]))\n",
    "    df[\"num_af_words\"] = df[\"aff_d\"].apply(lambda x: len(x))\n",
    "    df[\"sen_score\"] = df[\"text\"].apply(lambda x: TextBlob(x.decode(\"utf-8\").lower()).polarity)\n",
    "    df[\"swear_words\"] = df[\"tagged_words\"].apply(lambda x: check_swear_words(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_a_n(x):\n",
    "    for i in x:\n",
    "        if any(char.isdigit()for char in i[0]) and any(char.isalpha()for char in i[0]):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def check_not_vowel(x):\n",
    "    vowels = {\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"}\n",
    "    for i in x:\n",
    "        if not any(char in vowels for char in i[0]):\n",
    "            return 1 \n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_pr_dict_words(x):\n",
    "    count = 0\n",
    "    for word in x:\n",
    "        if word[0] in anc_all:\n",
    "            count +=1\n",
    "    return count * 1.0 / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_punctuation_d(x):\n",
    "    n_1 = x.count(\"!\")\n",
    "    n_2 = x.count(\".\")\n",
    "    n_3 = x.count(\",\")\n",
    "    n_4 = x.count('\"')\n",
    "    n_5 = x.count(\"'\")\n",
    "    n_6 = x.count(\"*\")\n",
    "    n_7 = x.count(\"?\")\n",
    "    return [n_1, n_2, n_3, n_4, n_5, n_6, n_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_grammar_f(df):\n",
    "    df = df.copy()\n",
    "    df[\"pos_d\"] = df[\"tagged_words\"].apply(lambda x: [pos_dict[i[1]] for i in x])\n",
    "    df[\"pos_d\"] = df[\"pos_d\"].apply(lambda x:get_distrbution(x, 36, -1))\n",
    "    df = get_multi_f(df, \"pos_d\", 36)\n",
    "    df[\"alphanumeric_words\"] = df[\"tagged_words\"].apply(lambda x: check_a_n(x))\n",
    "    df[\"w_o_vowels\"] = df[\"tagged_words\"].apply(lambda x: check_not_vowel(x))\n",
    "    df[\"pr_dict_word\"] = df[\"tagged_words\"].apply(lambda x: get_pr_dict_words(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def senti_score(x):\n",
    "    wl = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    if x in sentiments_dict:\n",
    "        return sentiments_dict[x]\n",
    "    lemma  = wl.lemmatize(x)\n",
    "    if lemma in sentiments_dict:\n",
    "        return sentiments_dict[lemma]\n",
    "    stem = ps.stem(x)\n",
    "    if stem in sentiments_dict:\n",
    "        return sentiments_dict[stem]\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_repeated_chars(text):\n",
    "    for i in range(len(text)-2):\n",
    "        if (text[i] == text[i+1]) and (text[i] == text[i+2]):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def check_repeated_in_sen(tagged_text):\n",
    "    for w in tagged_text:\n",
    "        if TextBlob(w[0] + \" \").polarity != 0:\n",
    "            if check_repeated_chars(w[0]):\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_written_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"rep_chars\"] = df[\"text\"].apply(lambda x: check_repeated_chars(x))\n",
    "    df[\"rep_chars_in_sen\"] = df[\"tagged_words\"].apply(lambda x: check_repeated_in_sen(x))\n",
    "    df[\"original_tagged_words\"] = df[\"text\"].apply(lambda x: TextBlob(x.decode(\"utf-8\"),pos_tagger=PatternTagger()).tags)\n",
    "    df[\"cap_words_number\"] = df[\"original_tagged_words\"].apply(lambda x: len([i for i in x if i[0].isupper()]))\n",
    "    df[\"cap_pos_d\"] = df[\"original_tagged_words\"].apply(lambda x: [pos_dict[i[1]] for i in x if i[0].isupper()])\n",
    "    df[\"cap_pos_d\"] = df[\"cap_pos_d\"].apply(lambda x:get_distrbution(x, 36, -1))\n",
    "    df = get_multi_f(df, \"cap_pos_d\", 36)\n",
    "    df[\"punct_dist\"] = df[\"text\"].apply(lambda x: get_punctuation_d(x))\n",
    "    df = get_multi_f(df, \"punct_dist\", 7)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_first_sent_l_word(x):\n",
    "    for ind, i in enumerate(x):\n",
    "        if TextBlob(i[0]+ \" \").polarity != 0:\n",
    "            return ind + 1\n",
    "    return 0\n",
    "\n",
    "def get_first_a_l_word(x):\n",
    "    for ind, i in enumerate(x):\n",
    "        if i[0] in affect_scores:\n",
    "            return ind + 1\n",
    "    return 0\n",
    "\n",
    "def get_density(x):\n",
    "    verbs = re.compile('VB*')\n",
    "    nouns = re.compile('NN*')\n",
    "    adjectives = re.compile('JJ*')\n",
    "    adverbs = re.compile('RB*')\n",
    "    l = [i for i in x if verbs.match(i[1]) or nouns.match(i[1]) or adjectives.match(i[1]) or\n",
    "         adverbs.match(i[1])]\n",
    "    return len(l) * 1.0 / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_structural_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"pos_1_w\"] = df[\"tagged_words\"].apply(lambda x: pos_dict[x[0][1]])\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"pos_1_w\"], prefix=\"pos_1_w_\" )], axis=1)\n",
    "    df = df.drop(\"pos_1_w\",axis=1)\n",
    "    df[\"pos_2_w\"] = df[\"tagged_words\"].apply(lambda x: pos_dict[x[1][1]])\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"pos_2_w\"], prefix=\"pos_2_w_\" )], axis=1)\n",
    "    df = df.drop(\"pos_2_w\",axis=1)\n",
    "    df[\"pos_3_w\"] = df[\"tagged_words\"].apply(lambda x: pos_dict[x[2][1]])\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"pos_3_w\"], prefix=\"pos_3_w_\" )], axis=1)\n",
    "    df = df.drop(\"pos_3_w\",axis=1)\n",
    "    df[\"pos_l1_w\"] = df[\"tagged_words\"].apply(lambda x: pos_dict[x[len(x)-1][1]])\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"pos_l1_w\"], prefix=\"pos_l1_w_\" )], axis=1)\n",
    "    df = df.drop(\"pos_l1_w\",axis=1)\n",
    "    df[\"pos_l2_w\"] = df[\"tagged_words\"].apply(lambda x: pos_dict[x[len(x)-2][1]])\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"pos_l2_w\"], prefix=\"pos_l2_w_\" )], axis=1)\n",
    "    df = df.drop(\"pos_l2_w\",axis=1)\n",
    "    df[\"pos_l3_w\"] = df[\"tagged_words\"].apply(lambda x: pos_dict[x[len(x)-3][1]])\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"pos_l3_w\"], prefix=\"pos_l3_w_\" )], axis=1)\n",
    "    df = df.drop(\"pos_l3_w\",axis=1)\n",
    "    df[\"p_1_s\"] = df[\"tagged_words\"].apply(lambda x: get_first_sent_l_word(x))\n",
    "    df[\"p_1_a\"] = df[\"tagged_words\"].apply(lambda x: get_first_a_l_word(x))\n",
    "    df[\"density\"] = df[\"tagged_words\"].apply(lambda x: get_density(x))\n",
    "    df[\"intens\"] = df[\"tagged_words\"].apply(lambda x: len([i for i in x if i[0] in booster_dict]))\n",
    "    df[\"pronouns\"] = df[\"tagged_words\"].apply(lambda x: len([i for i in x if i[1] == \"PRP\"]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"tagged_text\"] = df[\"text\"].apply(lambda x: get_tagging(x))\n",
    "    df[\"tagged_words\"] = df[\"text\"].apply(lambda x: TextBlob(x.encode(\"utf-8\").lower(),pos_tagger=PatternTagger()).tags)\n",
    "    df = get_contrasting_features(df)\n",
    "    df[\"length_d\"] = df[\"tagged_words\"].apply(lambda x: [len(i[0]) for i in x])\n",
    "    df = get_distribution_f(df, \"length_d\")\n",
    "    df = get_mood_f(df)\n",
    "    df = get_grammar_f(df)\n",
    "    df = get_written_features(df)\n",
    "    df = get_structural_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_f_small = create_features(data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(data_f_small.columns.difference([u'text', u'sarc', u'tagged_text',\n",
    "                          u'tagged_words','length_d','sen_d', 'aff_d', 'original_tagged_words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_trains_tests(data_f):\n",
    "    print \"w\"\n",
    "    skf = KFold(len(data_f), n_folds=10, shuffle=False, random_state=None)\n",
    "    trains = []\n",
    "    tests = []\n",
    "    data_1_1 = shuffle(data_f)\n",
    "    i = 0\n",
    "    for train, test in skf:\n",
    "        trains.append(data_f.iloc[train])\n",
    "        data_f.iloc[train][[\"text\", \"sarc\"]].to_csv(\"data_train_kf_\"+str(i)+\".csv\", encoding='utf-8',index=False)\n",
    "        tests.append(data_f.iloc[test])\n",
    "        data_f.iloc[test][[\"text\", \"sarc\"]].to_csv(\"data_test_kf_\"+str(i)+\".csv\", encoding='utf-8',index=False)\n",
    "        i += 1\n",
    "    return trains, tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scores(trains, tests, clf, features):\n",
    "    f_1_scores = []\n",
    "    roc_auc_scores = []\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    accuracy_scores = []\n",
    "    confusion_matrixes = []\n",
    "    y_prs = []\n",
    "    y_probas = []\n",
    "    i = 0\n",
    "    for train, test in tqdm(zip(trains,tests)):\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        x_train = scaler.fit_transform(train[features].astype(float))\n",
    "        y_train = train.sarc\n",
    "        x_test = scaler.transform(test[features].astype(float))\n",
    "        y_test = test.sarc\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pr = clf.predict(x_test)\n",
    "        \n",
    "        y_proba = clf.predict_proba(x_test)\n",
    "        pd.Series(list(y_pr)).to_csv(\"bm_y_prd_kf_\"+str(i)+\".csv\")\n",
    "        pd.Series(list(y_proba[:,1])).to_csv(\"bm_y_prb_kf_\"+str(i)+\".csv\")\n",
    "        i += 1\n",
    "        y_prs.append(y_pr)\n",
    "        y_probas.append(y_proba)\n",
    "        f_1_scores.append(f1_score(y_test, y_pr))\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, y_proba[:,1]))\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pr))\n",
    "        confusion_matrixes.append(confusion_matrix(y_test, y_pr))\n",
    "        recall_scores.append(recall_score(y_test, y_pr))\n",
    "        precision_scores.append(precision_score(y_test, y_pr))\n",
    "    return y_prs, y_probas, f_1_scores, roc_auc_scores, recall_scores,\\\n",
    "          precision_scores, accuracy_scores, confusion_matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n"
     ]
    }
   ],
   "source": [
    "trains_small, tests_small = get_trains_tests(data_f_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=550, n_jobs=-1,random_state=20)\n",
    "f_rf_prd, f_rf_prb, f1_small, auc_small, rec_small, pr_small, acc_small, con_small = get_scores(trains_small,\\\n",
    "                                                                                                tests_small, clf,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg f1: 0.745597540542\n",
      "avg roc_auc: 0.805304048707\n",
      "avg accuracy: 0.723020145016\n",
      "avg precision: 0.689175936033\n",
      "avg recall: 0.81292847442\n"
     ]
    }
   ],
   "source": [
    "print \"avg f1:\", np.mean(f1_small)\n",
    "print \"avg roc_auc:\", np.mean(auc_small)\n",
    "print \"avg accuracy:\", np.mean(acc_small)\n",
    "print \"avg precision:\", np.mean(pr_small)\n",
    "print \"avg recall:\", np.mean(rec_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "clf = svm.SVC(C=1.5, probability=True)\n",
    "f_svm_prd, f_svm_prb, f1_small, auc_small, rec_small, pr_small, acc_small, con_small = get_scores(trains_small, tests_small, clf, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg f1: 0.722693748569\n",
      "avg roc_auc: 0.792003697685\n",
      "avg accuracy: 0.714771201793\n",
      "avg precision: 0.703474185489\n",
      "avg recall: 0.744514558464\n"
     ]
    }
   ],
   "source": [
    "print \"avg f1:\", np.mean(f1_small)\n",
    "print \"avg roc_auc:\", np.mean(auc_small)\n",
    "print \"avg accuracy:\", np.mean(acc_small)\n",
    "print \"avg precision:\", np.mean(pr_small)\n",
    "print \"avg recall:\", np.mean(rec_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bag_of_words_test_trains(trains_data, tests_data):\n",
    "    trains = []\n",
    "    tests = []\n",
    "    y_trains = []\n",
    "    y_tests = []\n",
    "    for train, test in zip(trains_data, tests_data):\n",
    "        trigram_vectorizer = CountVectorizer(ngram_range=(1, 3), min_df=1)\n",
    "        X_train = trigram_vectorizer.fit_transform(train[\"text\"])\n",
    "        X_test = trigram_vectorizer.transform(test[\"text\"])\n",
    "        trains.append(X_train)\n",
    "        tests.append(X_test)\n",
    "        y_trains.append(train[\"sarc\"])\n",
    "        y_tests.append(test[\"sarc\"])\n",
    "    return trains, tests, y_trains, y_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores_bag(trains, tests, y_trains, y_tests, clf):\n",
    "    f_1_scores = []\n",
    "    roc_auc_scores = []\n",
    "    recall_scores = []\n",
    "    precision_scores = []\n",
    "    accuracy_scores = []\n",
    "    confusion_matrixes = []\n",
    "    y_pr_trains = []\n",
    "    y_proba_trains = []\n",
    "    y_prs = []\n",
    "    y_probas = []\n",
    "    i = 0\n",
    "    for train, test, y_train, y_test in tqdm(zip(trains, tests, y_trains, y_tests)):\n",
    "        clf.fit(train, y_train)\n",
    "        y_pr_train = clf.predict(train)\n",
    "        y_pr_trains.append(y_pr_train)\n",
    "        y_proba_train = clf.predict_proba(train)\n",
    "        y_proba_trains.append(y_proba_train[:,1])\n",
    "        y_pr = clf.predict(test)\n",
    "        y_prs.append(y_pr)\n",
    "        y_proba = clf.predict_proba(test)\n",
    "        pd.Series(list(y_pr)).to_csv(\"bag_y_prd_kf_\"+str(i)+\".csv\")\n",
    "        pd.Series(list(y_proba[:,1])).to_csv(\"bag_y_prb_kf_\"+str(i)+\".csv\")\n",
    "        y_probas.append(y_proba[:,1])\n",
    "        f_1_scores.append(f1_score(y_test, y_pr))\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, y_proba[:,1]))\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pr))\n",
    "        confusion_matrixes.append(confusion_matrix(y_test, y_pr))\n",
    "        recall_scores.append(recall_score(y_test, y_pr))\n",
    "        precision_scores.append(precision_score(y_test, y_pr))\n",
    "        i += 1\n",
    "    return y_pr_trains, y_proba_trains, y_prs, y_probas, f_1_scores, roc_auc_scores,\\\n",
    "           recall_scores, precision_scores, accuracy_scores, confusion_matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_trains, bag_tests, y_trains, y_tests = get_bag_of_words_test_trains(trains_small, tests_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.5)\n",
    "log_y_tr_prd, log_y_tr_prb, log_y_tst_prd, log_y_tst_prb, f1_small_2, auc_small_2, rec_small_2, pr_small_2,\\\n",
    "acc_small_2, con_small_2 = get_scores_bag(bag_trains, bag_tests, y_trains, y_tests, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg f1: 0.735144037597\n",
      "avg roc_auc: 0.812584746875\n",
      "avg accuracy: 0.735705379679\n",
      "avg precision: 0.736478061837\n",
      "avg recall: 0.734311108627\n"
     ]
    }
   ],
   "source": [
    "print \"avg f1:\", np.mean(f1_small_2)\n",
    "print \"avg roc_auc:\", np.mean(auc_small_2)\n",
    "print \"avg accuracy:\", np.mean(acc_small_2)\n",
    "print \"avg precision:\", np.mean(pr_small_2)\n",
    "print \"avg recall:\", np.mean(rec_small_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_data(trains, tests, y_tr_prds, y_tr_prbs, y_tst_prs, y_tst_prbs):\n",
    "    new_trains = []\n",
    "    new_tests = []\n",
    "    for tr, tst, y_tr_prd, y_tr_prb, y_tst_prd, y_tst_prb in zip(trains, tests, y_tr_prds,\\\n",
    "                                                                   y_tr_prbs, y_tst_prs, y_tst_prbs):\n",
    "        new_tr = tr.copy()\n",
    "        new_tst = tst.copy()\n",
    "        new_tr[\"y_prd\"] = y_tr_prd\n",
    "        new_tr[\"y_prb\"] = y_tr_prb\n",
    "        new_tst[\"y_prd\"] = y_tst_prd\n",
    "        new_tst[\"y_prb\"] = y_tst_prb\n",
    "        new_trains.append(new_tr)\n",
    "        new_tests.append(new_tst)\n",
    "    return new_trains, new_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_c_trains, data_c_tests = combine_data(trains_small, tests_small, y_tr_prd, y_tr_prb, y_tst_prd, y_tst_prb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=550, n_jobs=-1,random_state=20)\n",
    "m_prd, m_prb, mf1_small, mauc_small, mrec_small, mpr_small, macc_small, mcon_small = get_scores(data_c_trains, data_c_tests, clf, features + [\"y_prd\", \"y_prb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg f1: 0.733712879635\n",
      "avg roc_auc: 0.822723640974\n",
      "avg accuracy: 0.735695604763\n",
      "avg precision: 0.738551109879\n",
      "avg recall: 0.729625225378\n"
     ]
    }
   ],
   "source": [
    "print \"avg f1:\", np.mean(mf1_small)\n",
    "print \"avg roc_auc:\", np.mean(mauc_small)\n",
    "print \"avg accuracy:\", np.mean(macc_small)\n",
    "print \"avg precision:\", np.mean(mpr_small)\n",
    "print \"avg recall:\", np.mean(mrec_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans_scores2 = []\n",
    "for i in np.arange(0, 1.005, 0.01):\n",
    "    y_pr = [a[:,1] * i  + (1-i) * b for a, b in zip(f_rf_prb, log_y_tst_prb)]\n",
    "    ans_scores2.append([i, 1-i, [roc_auc_score(b, a) for a, b in zip(y_pr, y_tests)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc-mix: 0.83804054977\n"
     ]
    }
   ],
   "source": [
    "b = sorted(ans_scores2, key = lambda x: -np.array(x[2]).mean())[0]\n",
    "print \"auc-mix:\", np.mean(b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_and(a, b):\n",
    "    y = []\n",
    "    for i, j in zip(a, b):\n",
    "        if (i == 1) and (j == 1):\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    return y\n",
    "\n",
    "def get_or(a, b):\n",
    "    y = []\n",
    "    for i, j in zip(a, b):\n",
    "        if (i == 1) or (j == 1):\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pr = [get_and(a, b) for a, b in zip(f_rf_prd, log_y_tst_prd)]\n",
    "pr = [precision_score(b, a) for a, b in zip(y_pr, y_tests)]\n",
    "rec = [recall_score(b, a) for a, b in zip(y_pr, y_tests)]\n",
    "f1 = [f1_score(b, a) for a, b in zip(y_pr, y_tests)]\n",
    "acc = [accuracy_score(b, a) for a, b in zip(y_pr, y_tests)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix precision:  0.782735436921\n",
      "mix recall:  0.634784098998\n",
      "mix f1:  0.700494542161\n",
      "mix accuracy:  0.729066473775\n"
     ]
    }
   ],
   "source": [
    "print \"mix precision: \", np.mean(pr)\n",
    "print \"mix recall: \", np.mean(rec)\n",
    "print \"mix f1: \", np.mean(f1)\n",
    "print \"mix accuracy: \", np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pr = [get_or(a, b) for a, b in zip(f_rf_prd, log_y_tst_prd)]\n",
    "pr = [precision_score(b, a) for a, b in zip(y_pr, y_tests)]\n",
    "rec = [recall_score(b, a) for a, b in zip(y_pr, y_tests)]\n",
    "f1 = [f1_score(b, a) for a, b in zip(y_pr, y_tests)]\n",
    "acc = [accuracy_score(b, a) for a, b in zip(y_pr, y_tests)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix precision:  0.669749612503\n",
      "mix recall:  0.904914466224\n",
      "mix f1:  0.769375592831\n",
      "mix accuracy:  0.729208970332\n"
     ]
    }
   ],
   "source": [
    "print \"mix precision: \", np.mean(pr)\n",
    "print \"mix recall: \", np.mean(rec)\n",
    "print \"mix f1: \", np.mean(f1)\n",
    "print \"mix accuracy: \", np.mean(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
